{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens, phrases, or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)\n",
    " - NLP packages: NLTK, Gensim, spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1. Load the text for analysis\n",
    "\n",
    "text='''`strange days' chronicles the last two days of 1999 in los angeles. \n",
    " as the locals gear up for the new millenium , lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. \n",
    " he pines for his ex-girlfriend, faith (juliette lewis), but doesn't notice that another friend, mace (angela bassett) really cares for him. \n",
    " this film features good performances, impressive film-making technique and breath-taking crowd scenes. \n",
    " director kathryn bigelow knows her stuff and does not hesitate to use it. \n",
    " but as a whole, this is an unsatisfying movie. \n",
    " the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens))                   \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: breath-taking and film-making, doesn't\n",
    "# are split into two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. breath-taking and film-making each is captured as one word\n",
    "# doesn't becomes does n't\n",
    "# Pros: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.3 remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.4 NLTK's regular expression tokenizer (customizable)\n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as:\n",
    "# (1) must start with a word character  \n",
    "# (2) then contain zero or more word characters,\"-\", \n",
    "#     or \"'\" in the middle \n",
    "# (3) must end with a word character\n",
    "# e.g. film-making, doesn't\n",
    "\n",
    "pattern=r'\\w[\\w\\'-]*\\w'                        \n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.5 Use NLTK's regular expression tokenizer \n",
    "# to define sentences, i.e. \n",
    "# (1) starts with non-space character, \n",
    "# (2) contains any number of characters in the middle, \n",
    "#     as long as they are not \"!?.\"\n",
    "# (3) ends with !?.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Phrases: Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "bigrams=list(nltk.bigrams(tokens))  # tokens are created in Exercise 3.1.4\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, graduate students etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.1. Get collocation\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(tokens) # tokens are created in Exercise 3.1.4\n",
    "\n",
    "# the corpus is too small\n",
    "finder.nbest(bigram_measures.raw_freq, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# better?\n",
    "# most of them are in the pattern of \"xxx said\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 How to find collocations - PMI\n",
    "- By **frequency** (perhaps with filter)\n",
    "- **Pointwise Mutual Information (PMI)**\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appear once in the corpus? \n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.1.1 Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# load a built-in NLTK corpus as a list of words\n",
    "words=nltk.corpus.genesis.words('english-web.txt')\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "\n",
    "finder.apply_freq_filter(5)\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 How to find collocations - NPMI and others\n",
    "- **Normalized Pointwise Mutual Information (NPMI)**\n",
    "   - If $w_1$ and $w_2$ always occur together, i.e., $p(w_1)=p(w_2)=p(w_1,w_2)$, PMI reaches the maximum: $$PMI(w_1,w_2)=-\\log{p(w_1)}=-\\log{p(w_2)}=-\\log{p(w_1,w_2)}$$\n",
    "   - Normalized PMI is the PMI divided by the upper bound:\n",
    "   $$PMI(w_1,w_2)=\\frac{\\log{\\frac{p(w_1,w_2)}{(p(w_1)*p(w_2))}}}{-\\log{p(w_1,w_2)}}$$\n",
    "   \n",
    "- Another simple method by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "$$Score(w_1, w_2)=\\frac{count(w_1,w_2)-\\delta}{count(w_1)*count(w_2)}$$, where $\\delta$ is the minimum collocation frequency\n",
    "- Both methods are implemented in gensim package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5. Vocabulary \n",
    " - Vocabulary: the set of unique tokens (unigrams/phrases)  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word, Value: count of the word\n",
    "    * **nltk.FreqDist()**: a nice function for calculating frequncy of words/phrases\n",
    "        - Get the frequency of items in the parameter list \n",
    "        - Retruns an object similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "print(\"word_dist:\", word_dist)\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 10 words:\", word_dist.most_common(10))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5.1 Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.5.1.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "#print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words and\n",
    "                        word not in string.punctuation}\n",
    "\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "print(sorted(filtered_dict.items(), key=lambda item:-item[1]))\n",
    "\n",
    "print(len(filtered_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 positive/negative words: sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5.2.1\n",
    "# Find positive words \n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Naive sentiment analysis**:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals ...\"\n",
    "- How to deal with **negation**?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5.2.2 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if a positive word is preceded \n",
    "# by a negation within N words? \n",
    "# e.g. 'does not make any customer happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
