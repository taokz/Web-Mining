{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Topic Modeling </center>\n",
    "\n",
    "- This lecture is created based on \n",
    "    * Blei. D. (2012), \"Probabilistic Topic Models\", http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf\n",
    "    * http://videolectures.net/mlss09uk_blei_tm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\n",
    "  - Discover the hidden themes that pervade the collection.\n",
    "  - Annotate the documents according to those themes.\n",
    "  - Use annotations to organize, summarize, and search the texts.\n",
    "- Formal Definition\n",
    "<img src='topic_modeling.png' width='70%'>\n",
    "  - **Topics**: each topic is a **distribution over words** \n",
    "    - e.g. for topic \"Gentics\", $p('gene'~|~'Genetics')~=~0.04$, $p('dna'~|~'Genetics')=0.02$\n",
    "    - $K$ topics $\\theta_1, \\theta_2, ..., \\theta_K$, $N$ words $w_1, w_2, ..., w_N$ in corpus, we need to know  $p(w_i|\\theta_j)$ for $i \\in N$ and $j\\in K$\n",
    "  - **Document ($d$)**: a **mixture of topics**\n",
    "    - e.g. for above document $d$, $p('Genetics'~|~d)=0.5$, $p('LifeScience'~|~d)=0.15$, ...\n",
    "    - In general, given document $d$ and topic $\\theta_j$, we need to know $p(\\theta_j~|~d)$, i.e. **topic proportion**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Language Model\n",
    "- Definition: given a corpus with $M$ documents, $N$ words, $K$ topics, a model contains the following probabilities:\n",
    "  - topic probability distribution in corpus: <br>$p(\\theta_j)$ for $j \\in K$, $\\sum_{j\\in K}{p(\\theta_j)}=1$\n",
    "  - topic distribution per document $d$ (document assignment): <br>$p(\\theta_j~|~d)$, $\\sum_{j\\in K}{p(\\theta_j~|~d)}=1$ \n",
    "  - word distribution per topic (why do we need to know it?): <br> $p(w_i~|~\\theta_j)$ for $i \\in N$ and $j\\in K$, $\\sum_{i\\in N}{p(w_i~|~\\theta_j)}=1$ \n",
    "  <img src='language_model.png' width='30%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to estimate these probabilities?\n",
    "### 3.1. Supervised learning - Naive Bayes\n",
    "- Topic probability: \n",
    "$$ p(\\theta_j) = \\frac{\\text{documents in topic } j + \\alpha_1} {\\text{total documents}+\\alpha_2}$$  \n",
    "- Word distribution per topic: \n",
    "$$ p(w_i~|~\\theta_j)= \\frac{\\text{count of word } w_i \\text{ in topic } j + \\alpha_1} {\\text{total word count in documents of topic }j + \\alpha_2}$$  \n",
    "- Topic distribution per document: \n",
    "$$ \\begin{array}{l}\n",
    " p(\\theta_j~|~d) = \\frac{p(d|\\theta_j) * p(\\theta_j)}{p(d)} \\text{         # Bayesian rule}\\\\\n",
    " C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(d~|~\\theta)*p(\\theta)} \\text{         # maximum a posteriori}\\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(w_1,w_2, ...,w_N~|~\\theta)*p(\\theta)} \\\\\n",
    "    C_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}({\\prod_{i \\in N} {p(w_i~|~\\theta)})*p(\\theta)}  \\textit{ # independence assumption}\n",
    "  \\end{array}$$  \n",
    "- Naive Bayes model is also a kind of language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generative Model for Unsupervised learning \n",
    "- We don't have labeled data; we only observe the documents\n",
    "- We **cannot** estimate $p(\\theta_j)$ and $p(w_i~|~\\theta_j)$ as above\n",
    "- Instead, we use a **generative model** that describes how a document $d$ was created\n",
    "  1. decide on document length $N$, e.g. 100 words\n",
    "  2. decide on topic mixture (i.e. $p(\\theta_j~|~d)$), e.g. 70% about genetics and 30% about life science, ...\n",
    "  3. for each of the N words,\n",
    "     - 3.1. choose a topic from the topic mixture, e.g. \"genetics\"\n",
    "     - 3.2. choose a word from based on the probabilities of words in the topic (i.e. $p(w_i~|~\\theta_j)$), e.g. \"gene\"\n",
    "     - At the end, you may get a document such as \"gene dna life ...\"\n",
    "- We assume all documents in the dataset were generated following this process. Then we infer these probabilities from samples such that these probabilities have the maximum likelihood to generate the samples\n",
    "- Probabilities $p(w_i~|~\\theta_j)$ and $p(\\theta_j~|~d)$ are **hidden structures** to be discovered, a.k.a **latent variables**\n",
    "<img src='latent_structure.png' width='70%'>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Dirichlet Allocation (LDA)\n",
    "### 4.1 Model\n",
    "- A generative model which generates a document $d$ as follows:\n",
    "  1. Choose document length $N$ ∼ Poisson(ξ).\n",
    "  2. Choose topic mixture $\\theta$ ~ Dir(α).\n",
    "  3. For each of the $N$ words $w_n$:\n",
    "     - (a) Choose a topic assignment $z_n$ ∼ Multinomial(θ), i.e. $p(z_n~|~\\theta)$\n",
    "     - (b) Choose a word $w_n$ from the topic, $w_n$ ∼ Multinomial($\\beta_{z_n}$), where $\\beta_{z_n}$ is the word distribution for assigned topic $z_n$, i.e. $p(w_n~|~z_n, ~\\theta)$     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. A few distributions\n",
    "\n",
    "  - **Poisson**(ξ) : a given number of events occurring in a fixed interval of time/space with rate ξ independently of the time/space since the last event\n",
    "  - **Multinomial**(θ) & Multinomial($\\beta$): \n",
    "    - suppose X is a vector which represents n draws of a random variable with three possible outcomes (i.e. words), say A, B, C. \n",
    "    - e.g. when n=10, an example draw of X could be x = [4,4,2], i.e., A occured 4 times, B 4 times, and C 2 times \n",
    "    - assume three outcomes have probability $\\beta$={$\\beta_A$, $\\beta_B$, $\\beta_C$} respectively (i.e. 0.5,0.3,0.2)\n",
    "    - the multinomial distribution describes the prob. mass distribution of X, $$ P(X=[4,4,2]) = \\frac{10!}{4!4!2!}\\beta_A^{4}\\beta_B^{4}\\beta_C^{2}$$ \n",
    "  - **Dirichlet** Dir(α) : is a probability distribution with parameter $α, e.g. \\{α_1,α_2,α_3\\}$ to generate $θ, e.g. \\{ θ_1,θ_2,θ_3\\}$. For details of Dirichlet function, check videos e.g. https://www.youtube.com/watch?v=nfBNOWv1pgE \n",
    "  <img src='dirichlet.svg'>\n",
    "    - Dirichlet distribution is conjugate to the multinomial.\n",
    "    - Given a multinomial observation, the posterior distribution of θ is a Dirichlet.\n",
    "      - e.g. in the above multinomial example, if $\\beta$ ~ $Dir (\\alpha) $ (i.e. prior), with samples $X$, $\\beta~|~X$ ~ $Dir (\\alpha + X)$ (i.e. posterior)\n",
    "    - In LDA, usually $α_1=α_2=α_3=...=\\frac{1}{K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. LDA Parameter Fitting\n",
    "- Common techniques to estimate these probabilities are EM (Expectation-Maximization), Collapsed Gibbs Sampling (See Blei's paper for details)\n",
    "  - Collapsed Gibbs Sampling: https://www.youtube.com/watch?v=u7l5hhmdc0M\n",
    "    1. Randomly assign each word in each sample to a topic \n",
    "    2. Iteratively update the assignment of each word\n",
    "    <img src=\"gibbs_sampling.png\" width=\"70%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Evaluate Topic Model - Perplexity\n",
    "- For a single document $d$ with $N_d$ words $\\{w_1, w_2, ..., w_{N_d}\\}$, denoted as $\\textbf{W}_d$\n",
    "$$\n",
    "  perplexity(d)= exp({H(d)}),  \n",
    "  H(d) = - \\frac{ln (p(\\textbf{W}_d))}{N_d}  \n",
    "$$\n",
    "- $p(\\textbf{W}_d)$, the probability of seeing a document $d$, can be calculated based on:\n",
    "   - word distribution per topic, i.e. $p(w_i~|~\\theta_j)$, and \n",
    "   - topic mixture, i.e. $p(\\theta_j~|~d)$\n",
    "- For a test set of D with M documents\n",
    "$$ perplexity(d)= exp({H(D)}), H(D) = - \\frac{\\sum_{d \\in D} {ln   (p(\\textbf{W}_d)})}{\\sum_{d \\in D}{N_d}} $$\n",
    "- Intutition: \n",
    "  - A lower perplexity score indicates better generalization performance\n",
    "  - Minimizing H(d) is equivalent to maximizing log likelihood\n",
    "- To evaluate a topic model, calcuate perplexity on **testing dataset** (i.e. evaluating how generaalized the model is)\n",
    "- Note: if you have some labeled data, you should also conduct **external evaluation**, i.e. \n",
    "  - map each topic to a labeled class, \n",
    "  - compute precision/recall from the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiement with LDA\n",
    "- A few libraries available for LDA: gensim, lda, sklearn\n",
    "- We use sklearn here since it has a good text preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1. Load 20 news group data\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "\n",
    "data=json.load(open('../../dataset/ydata.json','r'))\n",
    "\n",
    "# shuffle the data\n",
    "shuffle(data)\n",
    "\n",
    "text,label=zip(*data)\n",
    "text=list(text)\n",
    "label=list(label)\n",
    "\n",
    "print(text[0])\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2. Preprocessing - Create Term Frequency Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LDA can only use raw term counts for LDA \n",
    "tf_vectorizer = CountVectorizer(max_df=0.90, \\\n",
    "                min_df=50, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "# each feature is a word (bag of words)\n",
    "# get_feature_names() gives all words\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(tf_feature_names[0:10])\n",
    "print(tf.shape)\n",
    "\n",
    "# split dataset into train (90%) and test sets (10%)\n",
    "# the test sets will be used to evaluate proplexity of topic modeling\n",
    "X_train, X_test = train_test_split(\\\n",
    "                tf, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3. Train LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 4\n",
    "\n",
    "# Run LDA. For details, check\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity\n",
    "\n",
    "# max_iter control the number of iterations \n",
    "# evaluate_every determines how often the perplexity is calculated\n",
    "# n_jobs is the number of parallel threads\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=20,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.4. Check topic and word distribution per topic\n",
    "\n",
    "num_top_words=20\n",
    "\n",
    "# lda.components_ returns a KxN matrix\n",
    "# for word distribution in each topic.\n",
    "# Each row consists of \n",
    "# probability (counts) of each word in the feature space\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    # print out top 20 words per topic \n",
    "    words=[(tf_feature_names[i],topic[i]) \\\n",
    "           for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "    print(words)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "num_top_words=50\n",
    "f, axarr = plt.subplots(2, 2, figsize=(8, 8));\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    # create a dataframe with two columns (word, weight) for each topic\n",
    "    \n",
    "    # create a word:count dictionary\n",
    "    f={tf_feature_names[i]:topic[i] for i in topic.argsort()[::-1][0:num_top_words]}\n",
    "    \n",
    "    # generate wordcloud in subplots\n",
    "    wordcloud = WordCloud(width=480, height=450, margin=0, background_color=\"black\");\n",
    "    _ = wordcloud.generate_from_frequencies(frequencies=f);\n",
    "    \n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].imshow(wordcloud, interpolation=\"bilinear\");\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].set_title(\"Topic: \"+str(topic_idx));\n",
    "    _ = axarr[math.floor(topic_idx/2), topic_idx%2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.5. Assign documents to topic\n",
    "import numpy as np\n",
    "\n",
    "# Generate topic assignment of each document\n",
    "topic_assign=lda.transform(X_train)\n",
    "\n",
    "print(topic_assign[0:5])\n",
    "\n",
    "# set a probability threshold\n",
    "# the threshold determines precision/recall\n",
    "prob_threshold=0.25\n",
    "\n",
    "topics=np.copy(topic_assign)\n",
    "topics=np.where(topics>=prob_threshold, 1, 0)\n",
    "print(topics[0:5])\n",
    "\n",
    "# How to calulate precision and recall\n",
    "# if your test data has been labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.6. Evaluate topic models by perplexity of test data\n",
    "\n",
    "perplexity=lda.perplexity(X_test)\n",
    "print(perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Find the number of topics ($K$)\n",
    "- There are no \"golden\" rules to find K.\n",
    "- Perplexity may be one way for you to find the number of topics\n",
    "    - Typically, the best number of topics should be around the **lowest perplexity**\n",
    "- However, in practice, a few factors need to be considered:\n",
    "  - It is usually difficult for human to understand or visulaize a big number of topics\n",
    "  - You may manually scan the data to figure out possible topics in the data, but these topics may not be correlated with the hidden structure discovered by LDA\n",
    "  - Usually, after LDA, we need manually inspect each discovered topic, merge or trim topics to get semantically coherent but distinguishable topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1.1 How to find the best number of topics?\n",
    "# Vary variable num_topics, e.g. set it to 2, 3, 5, ...\n",
    "# For each value, train LDA model, \n",
    "# calculate perplexity on the test data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result=[]\n",
    "for num_topics in range(2,15):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                learning_method='online', \\\n",
    "                                max_iter=10,verbose=0, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)\n",
    "    p=lda.perplexity(X_test)\n",
    "    result.append([num_topics,p])\n",
    "    print(num_topics, p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result, columns=[\"K\", \"Perlexity\"]).plot.line(x='K',y=\"Perlexity\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LDA Gensim Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.1. Create LDA model using the same TF matrix generated from sklearn\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# A corpus is TF matrix in the list format, e.g.:\n",
    "# [[(0, 1), (1,2), (4, 1), ...], [...], ...]\n",
    "# which shows the first document has words with id=0,1,4\n",
    "# and the count of word 0 is 1, word 1 is 2, ...\n",
    "\n",
    "# convert the gensim corpus from the sparse tf matrix\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_train, documents_columns=False)\n",
    "\n",
    "# create the mapping between id and words\n",
    "id2word={idx:w for idx, w in enumerate(tf_vectorizer.get_feature_names())}\n",
    "\n",
    "# create a gensim dictionary from the corpus\n",
    "# a dictionary contains the frequency of each words \n",
    "# the mapping between ids and words\n",
    "dictionary = corpora.Dictionary.from_corpus(corpus, id2word=id2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.2. Train LDA model\n",
    "\n",
    "NUM_TOPICS = 4\n",
    "\n",
    "# for detailed parameters, check\n",
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, alpha='auto', num_topics = NUM_TOPICS, \\\n",
    "                                           id2word=id2word, iterations=15)\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=20)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.3. visualize topics\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6.4. Test unseen documents\n",
    "\n",
    "test_corpus = gensim.matutils.Sparse2Corpus(X_test, documents_columns=False)\n",
    "predict = ldamodel.get_document_topics(test_corpus)\n",
    "list(predict)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
