{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deep Learning and Text Analytics</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutionary neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction neural networks\n",
    "- A neural network is a computational model inspired by the way biological neural networks in the human brain process information.\n",
    "- Neural networks have been widely applied in speech recognition, computer vision and text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Single Neuron\n",
    "\n",
    "<img src=\"single_neuron.png\" width=\"60%\">\n",
    "$$h_{W,b}(x)=f(w_1x_1+w_2x_2+w_3x_3+b)$$\n",
    "- Basic components:\n",
    "    - **input** ($X$): $[x_1, x_2, x_3]$\n",
    "    - **weight** ($W$): $[w_1, w_2, w_3]$\n",
    "    - **bias**: $b$\n",
    "    - **activation** function: $f$\n",
    "- Different activation functions:\n",
    "    - **Sigmoid** (logistic function): takes a real-valued input and squashes it to range [0,1]. $$f(z)=\\frac{1}{1+e^{-z}}$$, where $z=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "    - Tanh (hyperbolic tangent): takes a real-valued input and squashes it to the range [-1, 1]. $$f(z)=tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    - ReLU (Rectified Linear Unit): $$f(z)=max(0,z)$$   \n",
    "    - **Softmax** (normalized exponential function): a generalization of the logistic function. If $z=[z_1, z_2, ..., z_k]$ is a $k$-dimensional vector, $$f(z)_{j \\in k}=\\frac{e^{z_j}}{\\sum_{i=1}^k{e^{z_i}}}$$ \n",
    "     - $f(z)_{j} \\in [0,1]$\n",
    "     - $\\sum_{j \\in k} {f(z)_{j}} =1 $\n",
    "     - $f(z)_{j}$ is treated as the **probability** of component $j$, a probability distribution over $k$ different possible outcomes\n",
    "     - e.g. in multi-label classification, softmax gives a probability of each label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural Network Model\n",
    "- A neural network is composed of many simple neurons, so that the output of a neuron can be the input of another\n",
    "- The sample neural network model has 3 input nodes, 3 hidden units, and 1 output unit\n",
    "    - input layer: the leftmost layer\n",
    "    - outout layer: the rightmost layer (produce target, i.e. prediction, classification)\n",
    "    - bias units: indicated by \"+1\" node\n",
    "    - hidden layer: the middle layer of nodes\n",
    "<img src=\"neural_network.png\" width=\"60%\"/>\n",
    "\n",
    "- $W$, $x$, and $b$ usually represented as arrays (i.e. vectorized)\n",
    "   - $w_{ij}^{(l)}$: the weight associated with the link from unit $j$ in layer $l$ to unit $i$ in layer $l+1$\n",
    "   - $W^{(1)} \\in \\mathbb{R}^{3\\text{x}3}$, $W^{(2)} \\in \\mathbb{R}^{1\\text{x}3}$, $b^{(1)} \\in \\mathbb{R}^{3\\text{x}1}$, $b^{(2)} \\in \\mathbb{R}^{1\\text{x}1}$\n",
    "   - Note $W^{(l)}x$ is the dot product between $W^{(l)}$ and $x$, i.e. $W^{(l)} \\cdot x$\n",
    "   \n",
    "- If a neural network contains more than 1 hidden layer, it's called a **deep neural network** (**deep learning**)\n",
    "- Training a neural network model is to find $W$ and $b$ that optimize some **cost function**, given tranining samples (X,Y), where X and Y can be multi-dimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cost function\n",
    "- Training set: m samples denoted as $(X,Y)={(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "- A typical cost function: **mean_squared_error** \n",
    "  - Sum of square error: $J(W,b;x,y)=\\frac{1}{2}||h_{W,b}(x)-y||^2$\n",
    "  - Regularization (square of each weight, or L2): $\\sum_{i, j, l}(w_{ij}^{(l)})^2$. An important mechanism to prevent overfitting\n",
    "  - Cost function:\n",
    "$$J(W,b)=\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x)-y||^2)}+ \\frac{\\lambda}{2}\\sum_{i, j, l}(w_{ij}^{(l)})^2$$, where $\\lambda$ is **regularization coefficient**\n",
    "- Other popular cost functions\n",
    "  - **Cross-entropy cost**\n",
    "      - Let's assume a single neuron with sigmoid activation function <img src='single_neuron.png' width=\"30%\" style=\"float: right;\">\n",
    "      - Let $\\widehat y=h_{W,b}(x)$, the prediction of true value $y$. $\\widehat y, y \\in [0,1]$. \n",
    "      - Then cross-entrophy cost is defined as: $$J=-\\frac{1}{m}\\sum_{i=1}^m{y_i\\ln{\\widehat y_i}+(1-y_i)\\ln{(1-\\widehat y_i)}}$$\n",
    "      - What makes cross-entropy a good cost function\n",
    "        - It's non-negative\n",
    "        - if the neuron's output $\\widehat y$ is close to the actual value $y$ (0 or 1) for all training inputs, then the cross-entropy will be close to zero\n",
    "- For comparison between \"Sum of Square error\" and \"Cross-entropy cost\", read http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent\n",
    "- An optimization algorithm used to find the values of parameters ($W, b$) of a function ($J$) that minimizes a cost function ($J(W,b)$.\n",
    "- It is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm\n",
    "  <img src='gradient_descent.png' width='80%'>\n",
    "  resource: https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/\n",
    "- It uses derivatives of cost function to determine the direction to move the parameter values in order to get a lower cost on the next iteration\n",
    "- Procedure:\n",
    "    1. initialize $W$ with random values\n",
    "    2. given samples (X,Y) as inputs, calculate dirivatives of cost function with regard to every parameter $w_{ij}^{(l)}$, i.e. $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$\n",
    "    3. update parameters by $(w_{ij}^{(l)})^{'}=w_{ij}^{(l)}-\\alpha*\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$, where $\\alpha$ is the learning rate\n",
    "    4. repeat steps 2-3 until $w_{ij}^{(l)}$ converges\n",
    "- **Learning rate $\\alpha$**\n",
    "  - It's critical to pick the right learning rate. Big $\\alpha$ or small $\\alpha$?\n",
    "  - $\\alpha$ may need to be adapted as learning unfolds\n",
    "- Challenges of Gradient Descent\n",
    "  - It is expensive to compute $\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x_i)-y_i||^2)}$ for all samples in each round\n",
    "  - It is difficult to compute $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$ if a neural netowrk has many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Stochastic Gradient Descent\n",
    "- Estimate of cost function using a subset of randomly chosen training samples (mini-batch) instead of the entire training set\n",
    "- Procedure: \n",
    "  1. pick a randomly selected mini-batch, train with them and update $W, b$, \n",
    "  2. repeat step (1) with another randomly selected mini-batch until the training set is exhausted (i.e. complete an epoch), \n",
    "  3. start over with another epoch until $W, b$ converge\n",
    "- **Hyperparameters** (parameters that control the learning of $W, b$)\n",
    "    - **Batch size**: the size of samples selected for each iteration\n",
    "    - **Epoches**: One epoch means one complete pass through the whole training set. Ususally we need to use many epoches until $W, b$ converge\n",
    "    - e.g. if your sample size is 1000, and your batch size is 200, how many iterations are needed for one epoch?\n",
    "    - e.g. if you set # of epoches to 5, how many times in total you update $W, b$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Backpropagation Algorithm -- The efficient way to calcluate gradients (i.e. partial derivatives)\n",
    "\n",
    "Forward Propagation             |  Backprogation\n",
    ":-------------------------:|:-------------------------:\n",
    "![](forward-propagation.png)  |  ![](backpropagation.png)\n",
    "input signals are passing through each layer by multiplying the weights | backpropagate the error back to each layer proportional to perspective weights, and update the weights based on attributed errors in hope to correct the error\n",
    "- Algorithm:\n",
    "  1. perform a feedforward pass, computing the activations for layers L2, L3, ... and so on up to the output layer\n",
    "  2. for output layer $n$,<br> $\\delta^{(n)} = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " J(W,b; x, y) = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " \\frac{1}{2} \\left\\|y - h_{W,b}(x)\\right\\|^2 = - (y - a^{(n)}) \\cdot f'(z^{(n)})$\n",
    "  3. for $l=n-1, n-2, ..., n-3, ..., 2$,<br>\n",
    "  $ \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\cdot f'(z^{(l)})$\n",
    "  4. Compute the desired partial derivatives, which are given as:<br>\n",
    "     $ \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) = a^{(l)}_j \\delta_i^{(l+1)}$ <br>\n",
    "$\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) = \\delta_i^{(l+1)}$\n",
    "- Example: \n",
    "  - $\\delta^{(3)} = \\frac{\\partial}{\\partial z^{(3)}} J(W,b; x, y) = (a^{(3)} - y) \\cdot f'(z^{(3)})$\n",
    "\n",
    "  - $ \\delta^{(2)} = \\left((W^{(2)})^T \\delta^{(3)}\\right) \\cdot f'(z^{(2)})$\n",
    "  - $ \\frac{\\partial}{\\partial W_{12}^{(2)}} J(W,b; x, y) = a^{(2)}_2 \\delta_1^{(3)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameters\n",
    "- Hyperparameters are parameters that control the learning of $w, b$ (our learning target)\n",
    "- Summary of hyperparameters:\n",
    "    - Network structure:\n",
    "      - number of hidden layers\n",
    "      - number of neurons of each layer\n",
    "      - activation fucntion of each layer\n",
    "    - Learning rate ($\\alpha$)\n",
    "    - regularization coeffiecient ($\\lambda$)\n",
    "    - mini-batch size\n",
    "    - epoches\n",
    "- For detailed explanation, watch: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/TBvb5/parameters-vs-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Develop your First Neural Network Model with Keras\n",
    "- Keras: \n",
    "  - high-level library for neural network models\n",
    "  - It wraps the efficient numerical computation libraries Theano and TensorFlow \n",
    "- Why Keras:\n",
    "  - Simple to get started and keep going\n",
    "  - Written in python and higly modular; easy to expand\n",
    "  - Built-in modules for some sophisticated neural network models\n",
    "- Installation\n",
    "  - pip install keras (or pip install keras --upgrade if you already have it) to install the latest version \n",
    "  - pip install theano \n",
    "  - pip install tensorflow \n",
    "  - pip install np-utils \n",
    "- Basic procedure\n",
    "  1. Load data\n",
    "  2. Define model\n",
    "  3. Compile model\n",
    "  4. Fit model\n",
    "  5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic Keras Modeling Constructs\n",
    "- Sequential model:  linear stack of layers\n",
    "- Layers\n",
    "  - Dense: in a dense layer, each neuron is connected to neurons in the next layer\n",
    "  - Embedding\n",
    "  - Convolution\n",
    "  - MaxPooling\n",
    "  - ...\n",
    "- Cost (loss) functions\n",
    "  - mean_squared_error\n",
    "  - binary_crossentropy\n",
    "  - categorical_crossentropy\n",
    "  - ...\n",
    "- Optimizer (i.e. optimization algorithm)\n",
    "  - SGD (Stochastic Gradient Descent): fixed learning rate in all iterations\n",
    "  - Adagrad: adapts the learning rate to the parameters, performing larger updates for infrequent, and smaller updates for frequent parameters\n",
    "  - Adam (Adaptive Moment Estimation): computes adaptive learning rates for each parameter.\n",
    "- Metrics\n",
    "  - accuracy: a ratio of correctly predicted samples to the total samples\n",
    "  - precision/recall/f1 through sklearn package\n",
    "  - Example:\n",
    "    - acc: (90+85)/200=87%\n",
    "    - prec: \n",
    "    - recall:\n",
    "\n",
    "|        | Predicted T        |   Predicted F  |\n",
    "|:----------|-------------------:|---------------:|\n",
    "|Actual T  |  90                | 10              |\n",
    "|Actual F  |  15                | 85              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example\n",
    "- Example: build a simple neural network model to predict diabetes using \"Pima Indians onset of diabetes database\" at http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "  - Columns 1-8: variables \n",
    "  - Column 9: class variable, 0 or 1\n",
    "- A sequential model with 4 layers\n",
    "  - each node is a tensor, a function of multidimensional arrays\n",
    "    - Input (L1)\n",
    "    - L2 (hidden layer, dense)\n",
    "    - L3 (hidden layer, dense)\n",
    "    - Output (dense)\n",
    "  - the model is a tensor graph (computation graph)\n",
    "\n",
    "  <img src='model.png' width='20%'>\n",
    "  <div class=\"alert alert-block alert-info\">Training a deep learning model is a very empirical process. You may need to tune the hyperparameters in many iterations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1. Load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv(\"../../../dataset/pima-indians-diabetes.csv\",header=None)\n",
    "data.head()\n",
    "\n",
    "data[8].value_counts()\n",
    "\n",
    "X=data.values[:,0:8]\n",
    "y=data.values[:,8]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2. Create Model\n",
    "\n",
    "# sequential model is a linear stack of layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# in a dense layer which each neuron is connected to \n",
    "# each neuron in the next layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import packages for L2 regularization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# set lambda (regularization coefficient)\n",
    "lam=0.01\n",
    "\n",
    "# create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer with 12 neurons, 8 input variables\n",
    "# and rectifier activation function (relu)\n",
    "# and L2 regularization\n",
    "# how many parameters in this layer?\n",
    "model.add(Dense(12, input_dim=8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam), name='L2') )\n",
    "\n",
    "# add another hidden layer with 8 neurons\n",
    "model.add(Dense(8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam),name='L3') )\n",
    "\n",
    "# add the output layer with sigmoid activation function\n",
    "# to return probability\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "\n",
    "# compile the model using binary corss entropy cost function\n",
    "# adam optimizer and accuracy\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3. Check model configuration\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Show the model in a computation graph\n",
    "# it needs pydot and graphviz\n",
    "# don't worry if you don't have them installed\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4. Fit Model\n",
    "\n",
    "# train the model with min-batch of size 10, \n",
    "# 100 epoches (# how many iterations?)\n",
    "# Keep 20% samples for test\n",
    "# shuffle data before train-test split\n",
    "# set fitting history into variable \"training\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \\\n",
    "                                test_size=0.25, random_state=123)\n",
    "\n",
    "training=model.fit(X_train, y_train, \\\n",
    "                   validation_data=[X_test, y_test], \\\n",
    "                   shuffle=True,epochs=150, \\\n",
    "                   batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.5. Get prediction and performance\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# evaluate the model using samples\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], \\\n",
    "                        scores[1]*100))\n",
    "\n",
    "# get prediction\n",
    "predicted=model.predict(X_test)\n",
    "print(predicted[0:5])\n",
    "# reshape the 2-dimension array to 1-dimension\n",
    "predicted=np.reshape(predicted, -1)\n",
    "\n",
    "# decide prediction to be 1 or 0 based probability\n",
    "predicted=np.where(predicted>0.5, 1, 0)\n",
    "\n",
    "# calculate performance report\n",
    "print(metrics.classification_report(y_test, predicted, \\\n",
    "                                    labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
