{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deep Learning and Text Analytics</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutionary neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction neural networks\n",
    "- A neural network is a computational model inspired by the way biological neural networks in the human brain process information.\n",
    "- Neural networks have been widely applied in speech recognition, computer vision and text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Single Neuron\n",
    "\n",
    "<img src=\"single_neuron.png\" width=\"60%\">\n",
    "$$h_{W,b}(x)=f(w_1x_1+w_2x_2+w_3x_3+b)$$\n",
    "\n",
    "- Basic components:\n",
    "    - **input** ($X$): $[x_1, x_2, x_3]$\n",
    "    - **weight** ($W$): $[w_1, w_2, w_3]$\n",
    "    - **bias**: $b$\n",
    "    - **activation** function: $f$\n",
    "- Different activation functions:\n",
    "    - **Sigmoid** (logistic function): takes a real-valued input and squashes it to range [0,1]. $$f(z)=\\frac{1}{1+e^{-z}}$$, where $z=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "    - Tanh (hyperbolic tangent): takes a real-valued input and squashes it to the range [-1, 1]. $$f(z)=tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    - ReLU (Rectified Linear Unit): $$f(z)=max(0,z)$$   \n",
    "    - **Softmax** (normalized exponential function): a generalization of the logistic function. If $z=[z_1, z_2, ..., z_k]$ is a $k$-dimensional vector, $$f(z)_{j \\in k}=\\frac{e^{z_j}}{\\sum_{i=1}^k{e^{z_i}}}$$ \n",
    "     - $f(z)_{j} \\in [0,1]$\n",
    "     - $\\sum_{j \\in k} {f(z)_{j}} =1 $\n",
    "     - $f(z)_{j}$ is treated as the **probability** of component $j$, a probability distribution over $k$ different possible outcomes\n",
    "     - e.g. in multi-label classification, softmax gives a probability of each label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural Network Model\n",
    "- A neural network is composed of many simple neurons, so that the output of a neuron can be the input of another\n",
    "- The sample neural network model has 3 input nodes, 3 hidden units, and 1 output unit\n",
    "    - input layer: the leftmost layer\n",
    "    - outout layer: the rightmost layer (produce target, i.e. prediction, classification)\n",
    "    - bias units: indicated by \"+1\" node\n",
    "    - hidden layer: the middle layer of nodes\n",
    "<img src=\"neural_network.png\" width=\"60%\"/>\n",
    "\n",
    "- $W$, $x$, and $b$ usually represented as arrays (i.e. vectorized)\n",
    "   - $w_{ij}^{(l)}$: the weight associated with the link from unit $j$ in layer $l$ to unit $i$ in layer $l+1$\n",
    "   - $W^{(1)} \\in \\mathbb{R}^{3\\text{x}3}$, $W^{(2)} \\in \\mathbb{R}^{1\\text{x}3}$, $b^{(1)} \\in \\mathbb{R}^{3\\text{x}1}$, $b^{(2)} \\in \\mathbb{R}^{1\\text{x}1}$\n",
    "   - Note $W^{(l)}x$ is the dot product between $W^{(l)}$ and $x$, i.e. $W^{(l)} \\cdot x$\n",
    "   \n",
    "- If a neural network contains more than 1 hidden layer, it's called a **deep neural network** (**deep learning**)\n",
    "- Training a neural network model is to find $W$ and $b$ that optimize some **cost function**, given tranining samples (X,Y), where X and Y can be multi-dimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cost function\n",
    "- Training set: m samples denoted as $(X,Y)={(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "- A typical cost function: **mean_squared_error** \n",
    "  - Sum of square error: $J(W,b;x,y)=\\frac{1}{2}||h_{W,b}(x)-y||^2$\n",
    "  - Regularization (square of each weight, or L2): $\\sum_{i, j, l}(w_{ij}^{(l)})^2$. An important mechanism to prevent overfitting\n",
    "  - Cost function:\n",
    "$$J(W,b)=\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x)-y||^2)}+ \\frac{\\lambda}{2}\\sum_{i, j, l}(w_{ij}^{(l)})^2$$, where $\\lambda$ is **regularization coefficient**\n",
    "- Other popular cost functions\n",
    "  - **Cross-entropy cost**\n",
    "      - Let's assume a single neuron with sigmoid activation function <img src='single_neuron.png' width=\"30%\" style=\"float: right;\">\n",
    "      - Let $\\widehat y=h_{W,b}(x)$, the prediction of true value $y$. $\\widehat y, y \\in [0,1]$. \n",
    "      - Then cross-entrophy cost is defined as: $$J=-\\frac{1}{m}\\sum_{i=1}^m{y_i\\ln{\\widehat y_i}+(1-y_i)\\ln{(1-\\widehat y_i)}}$$\n",
    "      - What makes cross-entropy a good cost function\n",
    "        - It's non-negative\n",
    "        - if the neuron's output $\\widehat y$ is close to the actual value $y$ (0 or 1) for all training inputs, then the cross-entropy will be close to zero\n",
    "- For comparison between \"Sum of Square error\" and \"Cross-entropy cost\", read http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent\n",
    "- An optimization algorithm used to find the values of parameters ($W, b$) of a function ($J$) that minimizes a cost function ($J(W,b)$.\n",
    "- It is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm\n",
    "  <img src='gradient_descent.png' width='80%'>\n",
    "  resource: https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/\n",
    "- It uses derivatives of cost function to determine the direction to move the parameter values in order to get a lower cost on the next iteration\n",
    "- Procedure:\n",
    "    1. initialize $W$ with random values\n",
    "    2. given samples (X,Y) as inputs, calculate dirivatives of cost function with regard to every parameter $w_{ij}^{(l)}$, i.e. $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$\n",
    "    3. update parameters by $(w_{ij}^{(l)})^{'}=w_{ij}^{(l)}-\\alpha*\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$, where $\\alpha$ is the learning rate\n",
    "    4. repeat steps 2-3 until $w_{ij}^{(l)}$ converges\n",
    "- **Learning rate $\\alpha$**\n",
    "  - It's critical to pick the right learning rate. Big $\\alpha$ or small $\\alpha$?\n",
    "  - $\\alpha$ may need to be adapted as learning unfolds\n",
    "- Challenges of Gradient Descent\n",
    "  - It is expensive to compute $\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x_i)-y_i||^2)}$ for all samples in each round\n",
    "  - It is difficult to compute $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$ if a neural netowrk has many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Stochastic Gradient Descent\n",
    "- Estimate of cost function using a subset of randomly chosen training samples (mini-batch) instead of the entire training set\n",
    "- Procedure: \n",
    "  1. pick a randomly selected mini-batch, train with them and update $W, b$, \n",
    "  2. repeat step (1) with another randomly selected mini-batch until the training set is exhausted (i.e. complete an epoch), \n",
    "  3. start over with another epoch until $W, b$ converge\n",
    "- **Hyperparameters** (parameters that control the learning of $W, b$)\n",
    "    - **Batch size**: the size of samples selected for each iteration\n",
    "    - **Epoches**: One epoch means one complete pass through the whole training set. Ususally we need to use many epoches until $W, b$ converge\n",
    "    - e.g. if your sample size is 1000, and your batch size is 200, how many iterations are needed for one epoch?\n",
    "    - e.g. if you set # of epoches to 5, how many times in total you update $W, b$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Backpropagation Algorithm -- The efficient way to calcluate gradients (i.e. partial derivatives)\n",
    "\n",
    "Forward Propagation             |  Backprogation\n",
    ":-------------------------:|:-------------------------:\n",
    "![](forward-propagation.png)  |  ![](backpropagation.png)\n",
    "input signals are passing through each layer by multiplying the weights | backpropagate the error back to each layer proportional to perspective weights, and update the weights based on attributed errors in hope to correct the error\n",
    "- Algorithm:\n",
    "  1. perform a feedforward pass, computing the activations for layers L2, L3, ... and so on up to the output layer\n",
    "  2. for output layer $n$,<br> $\\delta^{(n)} = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " J(W,b; x, y) = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " \\frac{1}{2} \\left\\|y - h_{W,b}(x)\\right\\|^2 = - (y - a^{(n)}) \\cdot f'(z^{(n)})$\n",
    "  3. for $l=n-1, n-2, ..., n-3, ..., 2$,<br>\n",
    "  $ \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\cdot f'(z^{(l)})$\n",
    "  4. Compute the desired partial derivatives, which are given as:<br>\n",
    "     $ \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) = a^{(l)}_j \\delta_i^{(l+1)}$ <br>\n",
    "$\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) = \\delta_i^{(l+1)}$\n",
    "- Example: \n",
    "\n",
    "  - $\\delta^{(3)} = \\frac{\\partial}{\\partial z^{(3)}} J(W,b; x, y) = (a^{(3)} - y) \\cdot f'(z^{(3)})$\n",
    "\n",
    "  - $ \\delta^{(2)} = \\left((W^{(2)})^T \\delta^{(3)}\\right) \\cdot f'(z^{(2)})$\n",
    "  - $ \\frac{\\partial}{\\partial W_{12}^{(2)}} J(W,b; x, y) = a^{(2)}_2 \\delta_1^{(3)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameters\n",
    "- Hyperparameters are parameters that control the learning of $w, b$ (our learning target)\n",
    "- Summary of hyperparameters:\n",
    "    - Network structure:\n",
    "      - number of hidden layers\n",
    "      - number of neurons of each layer\n",
    "      - activation fucntion of each layer\n",
    "    - Learning rate ($\\alpha$)\n",
    "    - regularization coeffiecient ($\\lambda$)\n",
    "    - mini-batch size\n",
    "    - epoches\n",
    "- For detailed explanation, watch: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/TBvb5/parameters-vs-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Develop your First Neural Network Model with Keras\n",
    "- Keras: \n",
    "  - high-level library for neural network models\n",
    "  - It wraps the efficient numerical computation libraries Theano and TensorFlow \n",
    "- Why Keras:\n",
    "  - Simple to get started and keep going\n",
    "  - Written in python and higly modular; easy to expand\n",
    "  - Built-in modules for some sophisticated neural network models\n",
    "- Installation\n",
    "  - pip install keras (or pip install keras --upgrade if you already have it) to install the latest version \n",
    "  - pip install theano \n",
    "  - pip install tensorflow \n",
    "  - pip install np-utils \n",
    "- Basic procedure\n",
    "  1. Load data\n",
    "  2. Define model\n",
    "  3. Compile model\n",
    "  4. Fit model\n",
    "  5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic Keras Modeling Constructs\n",
    "- Sequential model:  linear stack of layers\n",
    "- Layers\n",
    "  - Dense: in a dense layer, each neuron is connected to neurons in the next layer\n",
    "  - Embedding\n",
    "  - Convolution\n",
    "  - MaxPooling\n",
    "  - ...\n",
    "- Cost (loss) functions\n",
    "  - mean_squared_error\n",
    "  - binary_crossentropy\n",
    "  - categorical_crossentropy\n",
    "  - ...\n",
    "- Optimizer (i.e. optimization algorithm)\n",
    "  - SGD (Stochastic Gradient Descent): fixed learning rate in all iterations\n",
    "  - Adagrad: adapts the learning rate to the parameters, performing larger updates for infrequent, and smaller updates for frequent parameters\n",
    "  - Adam (Adaptive Moment Estimation): computes adaptive learning rates for each parameter.\n",
    "- Metrics\n",
    "  - accuracy: a ratio of correctly predicted samples to the total samples\n",
    "  - precision/recall/f1 through sklearn package\n",
    "  - Example:\n",
    "    - acc: (90+85)/200=87%\n",
    "    - prec: \n",
    "    - recall:\n",
    "\n",
    "|        | Predicted T        |   Predicted F  |\n",
    "|:----------|-------------------:|---------------:|\n",
    "|Actual T  |  90                | 10              |\n",
    "|Actual F  |  15                | 85              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example\n",
    "- Example: build a simple neural network model to predict diabetes using \"Pima Indians onset of diabetes database\" at http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "  - Columns 1-8: variables \n",
    "  - Column 9: class variable, 0 or 1\n",
    "- A sequential model with 4 layers\n",
    "  - each node is a tensor, a function of multidimensional arrays\n",
    "    - Input (L1)\n",
    "    - L2 (hidden layer, dense)\n",
    "    - L3 (hidden layer, dense)\n",
    "    - Output (dense)\n",
    "  - the model is a tensor graph (computation graph)\n",
    "\n",
    "  <img src='model.png' width='20%'>\n",
    "  <div class=\"alert alert-block alert-info\">Training a deep learning model is a very empirical process. You may need to tune the hyperparameters in many iterations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.1. Load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv(\"pima-indians-diabetes.csv\",header=None)\n",
    "data.head()\n",
    "\n",
    "data[8].value_counts()\n",
    "\n",
    "X=data.values[:,0:8]\n",
    "y=data.values[:,8]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 8 4 9 1 6 7 3 0 5]\n",
      "[2 8 4 9 1 6 7 3 0 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "print (np.random.permutation(10)) \n",
    "np.random.seed(0) \n",
    "print (np.random.permutation(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2. Create Model\n",
    "\n",
    "# sequential model is a linear stack of layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# in a dense layer which each neuron is connected to \n",
    "# each neuron in the next layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import packages for L2 regularization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# set lambda (regularization coefficient)\n",
    "lam=0.01\n",
    "\n",
    "# create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer with 12 neurons, 8 input variables\n",
    "# and rectifier activation function (relu)\n",
    "# and L2 regularization\n",
    "# how many parameters in this layer?\n",
    "model.add(Dense(12, input_dim=8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam), name='L2') )\n",
    "\n",
    "# add another hidden layer with 8 neurons\n",
    "model.add(Dense(8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam),name='L3') )\n",
    "\n",
    "# add the output layer with sigmoid activation function\n",
    "# to return probability\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "\n",
    "# compile the model using binary corss entropy cost function\n",
    "# adam optimizer and accuracy\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "L2 (Dense)                   (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "L3 (Dense)                   (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3. Check model configuration\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Show the model in a computation graph\n",
    "# it needs pydot and graphviz\n",
    "# don't worry if you don't have them installed\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/150\n",
      " - 1s - loss: 5.0656 - acc: 0.6615 - val_loss: 4.8537 - val_acc: 0.6146\n",
      "Epoch 2/150\n",
      " - 0s - loss: 3.3151 - acc: 0.5955 - val_loss: 3.3636 - val_acc: 0.5208\n",
      "Epoch 3/150\n",
      " - 0s - loss: 1.8630 - acc: 0.5399 - val_loss: 1.5378 - val_acc: 0.5000\n",
      "Epoch 4/150\n",
      " - 0s - loss: 1.2645 - acc: 0.5799 - val_loss: 1.2476 - val_acc: 0.5312\n",
      "Epoch 5/150\n",
      " - 0s - loss: 1.0278 - acc: 0.6042 - val_loss: 1.0558 - val_acc: 0.5365\n",
      "Epoch 6/150\n",
      " - 0s - loss: 0.9128 - acc: 0.6181 - val_loss: 0.9431 - val_acc: 0.5938\n",
      "Epoch 7/150\n",
      " - 0s - loss: 0.8691 - acc: 0.6441 - val_loss: 0.9034 - val_acc: 0.5781\n",
      "Epoch 8/150\n",
      " - 0s - loss: 0.8426 - acc: 0.6476 - val_loss: 0.8840 - val_acc: 0.6094\n",
      "Epoch 9/150\n",
      " - 0s - loss: 0.8098 - acc: 0.6615 - val_loss: 0.8952 - val_acc: 0.6406\n",
      "Epoch 10/150\n",
      " - 0s - loss: 0.8002 - acc: 0.6944 - val_loss: 0.8703 - val_acc: 0.6458\n",
      "Epoch 11/150\n",
      " - 0s - loss: 0.7820 - acc: 0.6806 - val_loss: 0.8372 - val_acc: 0.6302\n",
      "Epoch 12/150\n",
      " - 0s - loss: 0.7662 - acc: 0.6944 - val_loss: 0.8283 - val_acc: 0.6615\n",
      "Epoch 13/150\n",
      " - 0s - loss: 0.7583 - acc: 0.6684 - val_loss: 0.8461 - val_acc: 0.6667\n",
      "Epoch 14/150\n",
      " - 0s - loss: 0.7539 - acc: 0.6649 - val_loss: 0.8641 - val_acc: 0.6458\n",
      "Epoch 15/150\n",
      " - 0s - loss: 0.7577 - acc: 0.6476 - val_loss: 0.8503 - val_acc: 0.6667\n",
      "Epoch 16/150\n",
      " - 0s - loss: 0.7288 - acc: 0.6910 - val_loss: 0.8083 - val_acc: 0.6510\n",
      "Epoch 17/150\n",
      " - 0s - loss: 0.7110 - acc: 0.6927 - val_loss: 0.7948 - val_acc: 0.6458\n",
      "Epoch 18/150\n",
      " - 0s - loss: 0.7323 - acc: 0.6719 - val_loss: 0.7669 - val_acc: 0.6927\n",
      "Epoch 19/150\n",
      " - 0s - loss: 0.7169 - acc: 0.6927 - val_loss: 0.7680 - val_acc: 0.6823\n",
      "Epoch 20/150\n",
      " - 0s - loss: 0.6967 - acc: 0.6997 - val_loss: 0.8045 - val_acc: 0.6510\n",
      "Epoch 21/150\n",
      " - 0s - loss: 0.6987 - acc: 0.6997 - val_loss: 0.7676 - val_acc: 0.6667\n",
      "Epoch 22/150\n",
      " - 0s - loss: 0.6904 - acc: 0.6875 - val_loss: 0.7656 - val_acc: 0.6875\n",
      "Epoch 23/150\n",
      " - 0s - loss: 0.6940 - acc: 0.6944 - val_loss: 0.7570 - val_acc: 0.6927\n",
      "Epoch 24/150\n",
      " - 0s - loss: 0.6788 - acc: 0.7257 - val_loss: 0.7439 - val_acc: 0.6823\n",
      "Epoch 25/150\n",
      " - 0s - loss: 0.6838 - acc: 0.7118 - val_loss: 0.7473 - val_acc: 0.6771\n",
      "Epoch 26/150\n",
      " - 0s - loss: 0.6800 - acc: 0.7031 - val_loss: 0.7295 - val_acc: 0.6875\n",
      "Epoch 27/150\n",
      " - 0s - loss: 0.6693 - acc: 0.7170 - val_loss: 0.7543 - val_acc: 0.6719\n",
      "Epoch 28/150\n",
      " - 0s - loss: 0.6667 - acc: 0.7135 - val_loss: 0.7307 - val_acc: 0.6979\n",
      "Epoch 29/150\n",
      " - 0s - loss: 0.6676 - acc: 0.7222 - val_loss: 0.7205 - val_acc: 0.6927\n",
      "Epoch 30/150\n",
      " - 0s - loss: 0.6710 - acc: 0.7274 - val_loss: 0.7374 - val_acc: 0.6823\n",
      "Epoch 31/150\n",
      " - 0s - loss: 0.6568 - acc: 0.7014 - val_loss: 0.7244 - val_acc: 0.6510\n",
      "Epoch 32/150\n",
      " - 0s - loss: 0.6671 - acc: 0.7066 - val_loss: 0.7496 - val_acc: 0.6615\n",
      "Epoch 33/150\n",
      " - 0s - loss: 0.6727 - acc: 0.7066 - val_loss: 0.7177 - val_acc: 0.6719\n",
      "Epoch 34/150\n",
      " - 0s - loss: 0.6617 - acc: 0.7101 - val_loss: 0.7311 - val_acc: 0.6719\n",
      "Epoch 35/150\n",
      " - 0s - loss: 0.6534 - acc: 0.7205 - val_loss: 0.7202 - val_acc: 0.6719\n",
      "Epoch 36/150\n",
      " - 0s - loss: 0.6508 - acc: 0.7205 - val_loss: 0.7239 - val_acc: 0.6458\n",
      "Epoch 37/150\n",
      " - 0s - loss: 0.6433 - acc: 0.7170 - val_loss: 0.7202 - val_acc: 0.6875\n",
      "Epoch 38/150\n",
      " - 0s - loss: 0.6478 - acc: 0.7153 - val_loss: 0.7235 - val_acc: 0.6719\n",
      "Epoch 39/150\n",
      " - 0s - loss: 0.6556 - acc: 0.7205 - val_loss: 0.7393 - val_acc: 0.6562\n",
      "Epoch 40/150\n",
      " - 0s - loss: 0.6478 - acc: 0.7274 - val_loss: 0.6953 - val_acc: 0.6979\n",
      "Epoch 41/150\n",
      " - 0s - loss: 0.6361 - acc: 0.7361 - val_loss: 0.7158 - val_acc: 0.6771\n",
      "Epoch 42/150\n",
      " - 0s - loss: 0.6453 - acc: 0.7188 - val_loss: 0.7373 - val_acc: 0.6771\n",
      "Epoch 43/150\n",
      " - 0s - loss: 0.6382 - acc: 0.7309 - val_loss: 0.7007 - val_acc: 0.6719\n",
      "Epoch 44/150\n",
      " - 0s - loss: 0.6321 - acc: 0.7118 - val_loss: 0.6920 - val_acc: 0.6979\n",
      "Epoch 45/150\n",
      " - 0s - loss: 0.6360 - acc: 0.7292 - val_loss: 0.6925 - val_acc: 0.6771\n",
      "Epoch 46/150\n",
      " - 0s - loss: 0.6357 - acc: 0.7292 - val_loss: 0.6926 - val_acc: 0.7083\n",
      "Epoch 47/150\n",
      " - 0s - loss: 0.6311 - acc: 0.7257 - val_loss: 0.6936 - val_acc: 0.6719\n",
      "Epoch 48/150\n",
      " - 0s - loss: 0.6320 - acc: 0.7274 - val_loss: 0.6844 - val_acc: 0.6979\n",
      "Epoch 49/150\n",
      " - 0s - loss: 0.6311 - acc: 0.7292 - val_loss: 0.6864 - val_acc: 0.6979\n",
      "Epoch 50/150\n",
      " - 0s - loss: 0.6295 - acc: 0.7292 - val_loss: 0.6813 - val_acc: 0.6979\n",
      "Epoch 51/150\n",
      " - 0s - loss: 0.6277 - acc: 0.7292 - val_loss: 0.6851 - val_acc: 0.6771\n",
      "Epoch 52/150\n",
      " - 0s - loss: 0.6386 - acc: 0.7188 - val_loss: 0.6930 - val_acc: 0.6771\n",
      "Epoch 53/150\n",
      " - 0s - loss: 0.6280 - acc: 0.7274 - val_loss: 0.6773 - val_acc: 0.7188\n",
      "Epoch 54/150\n",
      " - 0s - loss: 0.6255 - acc: 0.7309 - val_loss: 0.6975 - val_acc: 0.6875\n",
      "Epoch 55/150\n",
      " - 0s - loss: 0.6247 - acc: 0.7378 - val_loss: 0.6758 - val_acc: 0.6979\n",
      "Epoch 56/150\n",
      " - 0s - loss: 0.6322 - acc: 0.7309 - val_loss: 0.6708 - val_acc: 0.7240\n",
      "Epoch 57/150\n",
      " - 0s - loss: 0.6279 - acc: 0.7240 - val_loss: 0.6796 - val_acc: 0.7031\n",
      "Epoch 58/150\n",
      " - 0s - loss: 0.6189 - acc: 0.7274 - val_loss: 0.6737 - val_acc: 0.7292\n",
      "Epoch 59/150\n",
      " - 0s - loss: 0.6325 - acc: 0.7205 - val_loss: 0.6808 - val_acc: 0.6979\n",
      "Epoch 60/150\n",
      " - 0s - loss: 0.6206 - acc: 0.7240 - val_loss: 0.6878 - val_acc: 0.6615\n",
      "Epoch 61/150\n",
      " - 0s - loss: 0.6245 - acc: 0.7240 - val_loss: 0.6823 - val_acc: 0.6927\n",
      "Epoch 62/150\n",
      " - 0s - loss: 0.6405 - acc: 0.7170 - val_loss: 0.6909 - val_acc: 0.6979\n",
      "Epoch 63/150\n",
      " - 0s - loss: 0.6156 - acc: 0.7309 - val_loss: 0.6677 - val_acc: 0.7031\n",
      "Epoch 64/150\n",
      " - 0s - loss: 0.6126 - acc: 0.7448 - val_loss: 0.6904 - val_acc: 0.6979\n",
      "Epoch 65/150\n",
      " - 0s - loss: 0.6324 - acc: 0.7101 - val_loss: 0.6993 - val_acc: 0.6979\n",
      "Epoch 66/150\n",
      " - 0s - loss: 0.6328 - acc: 0.7309 - val_loss: 0.7011 - val_acc: 0.6823\n",
      "Epoch 67/150\n",
      " - 0s - loss: 0.6253 - acc: 0.7205 - val_loss: 0.7046 - val_acc: 0.6875\n",
      "Epoch 68/150\n",
      " - 0s - loss: 0.6340 - acc: 0.7066 - val_loss: 0.7474 - val_acc: 0.6667\n",
      "Epoch 69/150\n",
      " - 0s - loss: 0.6294 - acc: 0.7240 - val_loss: 0.6783 - val_acc: 0.6875\n",
      "Epoch 70/150\n",
      " - 0s - loss: 0.6128 - acc: 0.7309 - val_loss: 0.6763 - val_acc: 0.6979\n",
      "Epoch 71/150\n",
      " - 0s - loss: 0.6100 - acc: 0.7240 - val_loss: 0.6606 - val_acc: 0.7240\n",
      "Epoch 72/150\n",
      " - 0s - loss: 0.6173 - acc: 0.7292 - val_loss: 0.6895 - val_acc: 0.6719\n",
      "Epoch 73/150\n",
      " - 0s - loss: 0.6186 - acc: 0.7257 - val_loss: 0.6706 - val_acc: 0.6979\n",
      "Epoch 74/150\n",
      " - 0s - loss: 0.6328 - acc: 0.7222 - val_loss: 0.6829 - val_acc: 0.6771\n",
      "Epoch 75/150\n",
      " - 0s - loss: 0.6169 - acc: 0.7101 - val_loss: 0.6711 - val_acc: 0.7083\n",
      "Epoch 76/150\n",
      " - 0s - loss: 0.6115 - acc: 0.7326 - val_loss: 0.6563 - val_acc: 0.7188\n",
      "Epoch 77/150\n",
      " - 0s - loss: 0.6210 - acc: 0.7292 - val_loss: 0.6810 - val_acc: 0.6927\n",
      "Epoch 78/150\n",
      " - 0s - loss: 0.6136 - acc: 0.7326 - val_loss: 0.6895 - val_acc: 0.6875\n",
      "Epoch 79/150\n",
      " - 0s - loss: 0.6137 - acc: 0.7274 - val_loss: 0.6735 - val_acc: 0.7083\n",
      "Epoch 80/150\n",
      " - 0s - loss: 0.6052 - acc: 0.7431 - val_loss: 0.6613 - val_acc: 0.6979\n",
      "Epoch 81/150\n",
      " - 0s - loss: 0.6053 - acc: 0.7205 - val_loss: 0.6679 - val_acc: 0.6875\n",
      "Epoch 82/150\n",
      " - 0s - loss: 0.6033 - acc: 0.7257 - val_loss: 0.6665 - val_acc: 0.7031\n",
      "Epoch 83/150\n",
      " - 0s - loss: 0.6040 - acc: 0.7309 - val_loss: 0.6480 - val_acc: 0.7240\n",
      "Epoch 84/150\n",
      " - 0s - loss: 0.6040 - acc: 0.7188 - val_loss: 0.6577 - val_acc: 0.6979\n",
      "Epoch 85/150\n",
      " - 0s - loss: 0.6018 - acc: 0.7413 - val_loss: 0.6995 - val_acc: 0.7031\n",
      "Epoch 86/150\n",
      " - 0s - loss: 0.6022 - acc: 0.7344 - val_loss: 0.6700 - val_acc: 0.7031\n",
      "Epoch 87/150\n",
      " - 0s - loss: 0.6162 - acc: 0.7257 - val_loss: 0.6516 - val_acc: 0.7344\n",
      "Epoch 88/150\n",
      " - 0s - loss: 0.5974 - acc: 0.7326 - val_loss: 0.6509 - val_acc: 0.7240\n",
      "Epoch 89/150\n",
      " - 0s - loss: 0.5940 - acc: 0.7431 - val_loss: 0.6511 - val_acc: 0.7031\n",
      "Epoch 90/150\n",
      " - 0s - loss: 0.5984 - acc: 0.7326 - val_loss: 0.6538 - val_acc: 0.7135\n",
      "Epoch 91/150\n",
      " - 0s - loss: 0.6024 - acc: 0.7378 - val_loss: 0.6616 - val_acc: 0.7135\n",
      "Epoch 92/150\n",
      " - 0s - loss: 0.5945 - acc: 0.7431 - val_loss: 0.6442 - val_acc: 0.7188\n",
      "Epoch 93/150\n",
      " - 0s - loss: 0.5995 - acc: 0.7378 - val_loss: 0.6492 - val_acc: 0.7344\n",
      "Epoch 94/150\n",
      " - 0s - loss: 0.5985 - acc: 0.7240 - val_loss: 0.6511 - val_acc: 0.7240\n",
      "Epoch 95/150\n",
      " - 0s - loss: 0.5910 - acc: 0.7292 - val_loss: 0.6438 - val_acc: 0.7135\n",
      "Epoch 96/150\n",
      " - 0s - loss: 0.6063 - acc: 0.7153 - val_loss: 0.6413 - val_acc: 0.6979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/150\n",
      " - 0s - loss: 0.6014 - acc: 0.7465 - val_loss: 0.6581 - val_acc: 0.7083\n",
      "Epoch 98/150\n",
      " - 0s - loss: 0.5995 - acc: 0.7274 - val_loss: 0.6589 - val_acc: 0.6979\n",
      "Epoch 99/150\n",
      " - 0s - loss: 0.5968 - acc: 0.7413 - val_loss: 0.6950 - val_acc: 0.6875\n",
      "Epoch 100/150\n",
      " - 0s - loss: 0.6140 - acc: 0.7101 - val_loss: 0.7578 - val_acc: 0.6719\n",
      "Epoch 101/150\n",
      " - 0s - loss: 0.6115 - acc: 0.7309 - val_loss: 0.7350 - val_acc: 0.6615\n",
      "Epoch 102/150\n",
      " - 0s - loss: 0.6160 - acc: 0.7222 - val_loss: 0.6782 - val_acc: 0.7031\n",
      "Epoch 103/150\n",
      " - 0s - loss: 0.5984 - acc: 0.7344 - val_loss: 0.6509 - val_acc: 0.7188\n",
      "Epoch 104/150\n",
      " - 0s - loss: 0.5822 - acc: 0.7483 - val_loss: 0.6585 - val_acc: 0.7083\n",
      "Epoch 105/150\n",
      " - 0s - loss: 0.5868 - acc: 0.7448 - val_loss: 0.6408 - val_acc: 0.7188\n",
      "Epoch 106/150\n",
      " - 0s - loss: 0.5870 - acc: 0.7535 - val_loss: 0.6655 - val_acc: 0.7031\n",
      "Epoch 107/150\n",
      " - 0s - loss: 0.5881 - acc: 0.7396 - val_loss: 0.6407 - val_acc: 0.7292\n",
      "Epoch 108/150\n",
      " - 0s - loss: 0.5852 - acc: 0.7517 - val_loss: 0.6762 - val_acc: 0.6927\n",
      "Epoch 109/150\n",
      " - 0s - loss: 0.5870 - acc: 0.7326 - val_loss: 0.6407 - val_acc: 0.7240\n",
      "Epoch 110/150\n",
      " - 0s - loss: 0.5891 - acc: 0.7344 - val_loss: 0.6425 - val_acc: 0.7344\n",
      "Epoch 111/150\n",
      " - 0s - loss: 0.5965 - acc: 0.7222 - val_loss: 0.6306 - val_acc: 0.7344\n",
      "Epoch 112/150\n",
      " - 0s - loss: 0.5840 - acc: 0.7378 - val_loss: 0.6323 - val_acc: 0.7344\n",
      "Epoch 113/150\n",
      " - 0s - loss: 0.5888 - acc: 0.7396 - val_loss: 0.6365 - val_acc: 0.7240\n",
      "Epoch 114/150\n",
      " - 0s - loss: 0.5777 - acc: 0.7500 - val_loss: 0.6334 - val_acc: 0.7240\n",
      "Epoch 115/150\n",
      " - 0s - loss: 0.5814 - acc: 0.7396 - val_loss: 0.6324 - val_acc: 0.7448\n",
      "Epoch 116/150\n",
      " - 0s - loss: 0.5829 - acc: 0.7465 - val_loss: 0.6613 - val_acc: 0.7188\n",
      "Epoch 117/150\n",
      " - 0s - loss: 0.5830 - acc: 0.7413 - val_loss: 0.6485 - val_acc: 0.7031\n",
      "Epoch 118/150\n",
      " - 0s - loss: 0.5764 - acc: 0.7378 - val_loss: 0.6315 - val_acc: 0.7396\n",
      "Epoch 119/150\n",
      " - 0s - loss: 0.5814 - acc: 0.7378 - val_loss: 0.6503 - val_acc: 0.7188\n",
      "Epoch 120/150\n",
      " - 0s - loss: 0.5786 - acc: 0.7465 - val_loss: 0.6259 - val_acc: 0.7240\n",
      "Epoch 121/150\n",
      " - 0s - loss: 0.5777 - acc: 0.7257 - val_loss: 0.6404 - val_acc: 0.7344\n",
      "Epoch 122/150\n",
      " - 0s - loss: 0.5798 - acc: 0.7344 - val_loss: 0.6324 - val_acc: 0.7240\n",
      "Epoch 123/150\n",
      " - 0s - loss: 0.5720 - acc: 0.7500 - val_loss: 0.6361 - val_acc: 0.7448\n",
      "Epoch 124/150\n",
      " - 0s - loss: 0.5985 - acc: 0.7240 - val_loss: 0.6441 - val_acc: 0.7396\n",
      "Epoch 125/150\n",
      " - 0s - loss: 0.6027 - acc: 0.7326 - val_loss: 0.6382 - val_acc: 0.7083\n",
      "Epoch 126/150\n",
      " - 0s - loss: 0.5787 - acc: 0.7448 - val_loss: 0.6472 - val_acc: 0.7083\n",
      "Epoch 127/150\n",
      " - 0s - loss: 0.5862 - acc: 0.7309 - val_loss: 0.6363 - val_acc: 0.7135\n",
      "Epoch 128/150\n",
      " - 0s - loss: 0.5778 - acc: 0.7413 - val_loss: 0.6350 - val_acc: 0.7188\n",
      "Epoch 129/150\n",
      " - 0s - loss: 0.5706 - acc: 0.7517 - val_loss: 0.6285 - val_acc: 0.7188\n",
      "Epoch 130/150\n",
      " - 0s - loss: 0.5783 - acc: 0.7517 - val_loss: 0.6429 - val_acc: 0.7344\n",
      "Epoch 131/150\n",
      " - 0s - loss: 0.5765 - acc: 0.7500 - val_loss: 0.6330 - val_acc: 0.7292\n",
      "Epoch 132/150\n",
      " - 0s - loss: 0.5694 - acc: 0.7483 - val_loss: 0.6318 - val_acc: 0.7135\n",
      "Epoch 133/150\n",
      " - 0s - loss: 0.5823 - acc: 0.7431 - val_loss: 0.6491 - val_acc: 0.7344\n",
      "Epoch 134/150\n",
      " - 0s - loss: 0.5710 - acc: 0.7517 - val_loss: 0.6227 - val_acc: 0.7135\n",
      "Epoch 135/150\n",
      " - 0s - loss: 0.5654 - acc: 0.7535 - val_loss: 0.6278 - val_acc: 0.7292\n",
      "Epoch 136/150\n",
      " - 0s - loss: 0.5677 - acc: 0.7535 - val_loss: 0.6184 - val_acc: 0.7240\n",
      "Epoch 137/150\n",
      " - 0s - loss: 0.5850 - acc: 0.7344 - val_loss: 0.6265 - val_acc: 0.7396\n",
      "Epoch 138/150\n",
      " - 0s - loss: 0.5718 - acc: 0.7517 - val_loss: 0.6203 - val_acc: 0.7188\n",
      "Epoch 139/150\n",
      " - 0s - loss: 0.5632 - acc: 0.7587 - val_loss: 0.6363 - val_acc: 0.7292\n",
      "Epoch 140/150\n",
      " - 0s - loss: 0.5774 - acc: 0.7431 - val_loss: 0.6289 - val_acc: 0.7135\n",
      "Epoch 141/150\n",
      " - 0s - loss: 0.5692 - acc: 0.7483 - val_loss: 0.6226 - val_acc: 0.7240\n",
      "Epoch 142/150\n",
      " - 0s - loss: 0.5759 - acc: 0.7361 - val_loss: 0.6269 - val_acc: 0.7552\n",
      "Epoch 143/150\n",
      " - 0s - loss: 0.5857 - acc: 0.7274 - val_loss: 0.6307 - val_acc: 0.7344\n",
      "Epoch 144/150\n",
      " - 0s - loss: 0.5796 - acc: 0.7500 - val_loss: 0.6252 - val_acc: 0.7135\n",
      "Epoch 145/150\n",
      " - 0s - loss: 0.5672 - acc: 0.7604 - val_loss: 0.6496 - val_acc: 0.7292\n",
      "Epoch 146/150\n",
      " - 0s - loss: 0.5733 - acc: 0.7344 - val_loss: 0.6668 - val_acc: 0.7188\n",
      "Epoch 147/150\n",
      " - 0s - loss: 0.5824 - acc: 0.7465 - val_loss: 0.6832 - val_acc: 0.6979\n",
      "Epoch 148/150\n",
      " - 0s - loss: 0.5699 - acc: 0.7448 - val_loss: 0.6479 - val_acc: 0.7240\n",
      "Epoch 149/150\n",
      " - 0s - loss: 0.5671 - acc: 0.7483 - val_loss: 0.6194 - val_acc: 0.7344\n",
      "Epoch 150/150\n",
      " - 0s - loss: 0.5791 - acc: 0.7396 - val_loss: 0.6139 - val_acc: 0.7292\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.4. Fit Model\n",
    "\n",
    "# train the model with min-batch of size 10, \n",
    "# 100 epoches (# how many iterations?)\n",
    "# Keep 20% samples for test\n",
    "# shuffle data before train-test split\n",
    "# set fitting history into variable \"training\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \\\n",
    "                                test_size=0.25, random_state=123)\n",
    "\n",
    "training=model.fit(X_train, y_train, \\\n",
    "                   validation_data=[X_test, y_test], \\\n",
    "                   shuffle=True,epochs=150, \\\n",
    "                   batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 139us/step\n",
      "\n",
      "acc: 72.92%\n",
      "[[0.76005745]\n",
      " [0.46581355]\n",
      " [0.7090628 ]\n",
      " [0.2537937 ]\n",
      " [0.11607096]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.81      0.79       119\n",
      "          1       0.66      0.60      0.63        73\n",
      "\n",
      "avg / total       0.73      0.73      0.73       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5. Get prediction and performance\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# evaluate the model using samples\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], \\\n",
    "                        scores[1]*100))\n",
    "\n",
    "# get prediction\n",
    "predicted=model.predict(X_test)\n",
    "print(predicted[0:5])\n",
    "# reshape the 2-dimension array to 1-dimension\n",
    "predicted=np.reshape(predicted, -1)\n",
    "\n",
    "# decide prediction to be 1 or 0 based probability\n",
    "predicted=np.where(predicted>0.5, 1, 0)\n",
    "\n",
    "# calculate performance report\n",
    "print(metrics.classification_report(y_test, predicted, \\\n",
    "                                    labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
